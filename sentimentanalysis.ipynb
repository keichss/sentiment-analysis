{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Uocn-fwnye-"
      },
      "source": [
        "### Импорт библиотек\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "912I5pQOb4TA"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from collections import Counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wX3p4k0A7QY"
      },
      "source": [
        "### Считывание данных"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n = ['text']\n",
        "data_positive = pd.read_csv('positive.csv', sep=';', names=n, usecols=['text'])\n",
        "data_negative = pd.read_csv('negative.csv', sep=';', names=n, usecols=['text'])"
      ],
      "metadata": {
        "id": "t1AuDhvoYaDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4OKkcDAA3JR"
      },
      "source": [
        "### Формирование датасета"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GuFziKescpdk"
      },
      "source": [
        "sample_size_neg = 40000\n",
        "sample_size_pos = 40000\n",
        "reviews_withoutshuffle = np.concatenate((data_positive['text'].values[:sample_size_pos],\n",
        "                           data_negative['text'].values[:sample_size_neg]), axis=0)\n",
        "labels_withoutshuffle = np.asarray([1] * sample_size_pos + [0] * sample_size_neg)\n",
        "\n",
        "assert len(reviews_withoutshuffle) == len(labels_withoutshuffle)\n",
        "from sklearn.utils import shuffle\n",
        "reviews,labels = shuffle(reviews_withoutshuffle, labels_withoutshuffle, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odsDQSD6Fwse"
      },
      "source": [
        "### Алгоритм предварительной обработки"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pymorphy3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yanccVpyDEsp",
        "outputId": "38253efd-28c2-489f-e235-4cc56a69df16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.1-py3-none-any.whl (53 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/53.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.5/53.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m862.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dawg-python>=0.7.1 (from pymorphy3)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg-python, pymorphy3\n",
            "Successfully installed dawg-python-0.7.2 pymorphy3-2.0.1 pymorphy3-dicts-ru-2.4.417150.4580142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import string\n",
        "import pymorphy3\n",
        "#from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "UWoZhP0PDPsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C4abUodxDSOQ",
        "outputId": "f04c56f0-fa99-45bb-d7c6-82e5bf5ab1a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stop_words = set(stopwords.words('russian'))\n",
        "morph = pymorphy3.MorphAnalyzer()"
      ],
      "metadata": {
        "id": "ngMhCZhdDcbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rus_stopwords = {'того', 'вам', 'перед', 'можно', 'нее', 'а', 'быть', 'и', 'один', 'ему', 'будто', 'меня', 'если', 'им', 'ну', 'ли', 'никогда', 'для', 'под', 'два', 'была', 'то', 'со', 'чуть', 'вы', 'чтоб', 'от', 'нибудь', 'разве', 'опять', 'после', 'ни', 'все', 'же', 'всех', 'более', 'нельзя', 'там', 'есть', 'чего', 'моя', 'тоже', 'ведь', 'уже', 'впрочем', 'конечно', 'ж', 'них', 'нет', 'зачем', 'так', 'ее', 'тем', 'тогда', 'себе', 'их', 'чтобы', 'больше', 'даже', 'такой', 'через', 'нас', 'ней', 'вот', 'они', 'хоть', 'при', 'бы', 'совсем', 'ничего', 'мы', 'ты', 'только', 'какая', 'теперь', 'где', 'этого', 'здесь', 'наконец', 'эту', 'ей', 'свою', 'может', 'надо', 'будет', 'этот', 'всего', 'потом', 'другой', 'эти', 'у', 'тут', 'какой', 'мой', 'ним', 'она', 'хорошо', 'раз', 'него', 'по', 'куда', 'между', 'о', 'как', 'я', 'вас', 'тебя', 'были', 'вдруг', 'про', 'этом', 'к', 'потому', 'был', 'да', 'над', 'почти', 'его', 'сам', 'что', 'но', 'или', 'себя', 'из', 'до', 'тот', 'лучше', 'уж', 'три', 'когда', 'за', 'иногда', 'во', 'сейчас', 'много', 'еще', 'мне', 'чем', 'на', 'было', 'всегда', 'кто', 'в', 'об', 'он', 'с', 'всю', 'без', 'этой', 'том'}\n",
        "print(rus_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNRPaqbz2Sk5",
        "outputId": "440bd8b7-e1ab-44fa-b2cb-fc6b8db571f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'со', 'от', 'к', 'них', 'того', 'много', 'по', 'этого', 'там', 'какой', 'после', 'им', 'всех', 'хоть', 'меня', 'или', 'будто', 'при', 'тот', 'этот', 'из', 'впрочем', 'с', 'чтобы', 'моя', 'перед', 'тогда', 'где', 'нее', 'один', 'всего', 'чтоб', 'между', 'нет', 'во', 'всю', 'она', 'ли', 'больше', 'ж', 'еще', 'были', 'ничего', 'его', 'потому', 'нельзя', 'нас', 'про', 'здесь', 'есть', 'хорошо', 'конечно', 'уже', 'ним', 'без', 'об', 'все', 'ней', 'тоже', 'то', 'другой', 'у', 'эту', 'да', 'до', 'если', 'зачем', 'никогда', 'был', 'мой', 'за', 'иногда', 'ты', 'чуть', 'два', 'бы', 'ему', 'ни', 'ее', 'раз', 'этой', 'вы', 'себя', 'разве', 'и', 'мы', 'в', 'что', 'теперь', 'сам', 'куда', 'чего', 'тем', 'вот', 'так', 'надо', 'над', 'мне', 'можно', 'будет', 'такой', 'нибудь', 'было', 'даже', 'эти', 'какая', 'они', 'вдруг', 'быть', 'под', 'тут', 'тебя', 'него', 'как', 'чем', 'а', 'может', 'вас', 'для', 'почти', 'когда', 'себе', 'ей', 'я', 'опять', 'их', 'сейчас', 'том', 'наконец', 'свою', 'но', 'вам', 'только', 'он', 'лучше', 'три', 'более', 'о', 'была', 'всегда', 'ведь', 'потом', 'же', 'через', 'на', 'уж', 'этом', 'ну', 'совсем', 'кто'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbg32M69b3Zn"
      },
      "source": [
        "def tokenize():\n",
        "    punctuation = '!\"#$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~№'\n",
        "    all_reviews = ' separator '.join(reviews)\n",
        "    all_reviews = all_reviews.lower()\n",
        "    all_text = ''.join([c for c in all_reviews if c not in punctuation])\n",
        "\n",
        "    low_tokens = all_text.split()\n",
        "    prep_text = []\n",
        "    for token in low_tokens:\n",
        "        lemma = morph.parse(token)[0].normal_form\n",
        "        if lemma not in rus_stopwords:\n",
        "            prep_text.append(lemma)\n",
        "    all_text = ' '.join(prep_text)\n",
        "    texts_split = all_text.split('separator')\n",
        "    all_text = ' '.join(texts_split)\n",
        "    words = all_text.split()\n",
        "    return words, texts_split\n",
        "\n",
        "words, texts_split = tokenize()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI5a5m7IekFq",
        "collapsed": true
      },
      "source": [
        "new_reviews = []\n",
        "for review in texts_split:\n",
        "    review = review.split()\n",
        "    new_text = []\n",
        "    for word in review:\n",
        "        if (word[0] != '@') & ('http' not in word) & (~word.isdigit()):\n",
        "            new_text.append(word)\n",
        "    new_reviews.append(new_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4u0eUxPevHv"
      },
      "source": [
        "counts = Counter(words)\n",
        "vocab = sorted(counts, key=counts.get, reverse=True)\n",
        "vocab_to_int = {word: ii - 1 for ii, word in enumerate(vocab, 1)}\n",
        "reviews_ints = []\n",
        "for review in new_reviews:\n",
        "    reviews_ints.append([vocab_to_int[word] for word in review])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyQIhUcWf0MI"
      },
      "source": [
        "def add_pads(reviews_ints, seq_length):\n",
        "    features = np.zeros((len(reviews_ints), seq_length), dtype=int)\n",
        "    for i, row in enumerate(reviews_ints):\n",
        "        if len(row) == 0:\n",
        "            continue\n",
        "        features[i, -len(row):] = np.array(row)[:seq_length]\n",
        "    return features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbKei28Ymt7U"
      },
      "source": [
        "### Разделение на обучающую, валидационную и тестовую выборки"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJIHlPHYgFeu"
      },
      "source": [
        "features = add_pads(reviews_ints, seq_length=50)\n",
        "split_frac = 0.8 # 80% на обучающую выборку\n",
        "\n",
        "split_idx = int(len(features)*split_frac)\n",
        "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
        "train_y, remaining_y = labels[:split_idx], labels[split_idx:]\n",
        "test_idx = int(len(remaining_x)*0.5)\n",
        "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
        "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
        "\n",
        "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
        "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
        "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
        "batch_size = 50\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
        "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lK3HUhBcoDgy"
      },
      "source": [
        "### Определение режима: GPU или CPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xTZURLbfGk9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32f84c9e-b056-4889-b9ec-5bd689370c58"
      },
      "source": [
        "train_gpu=torch.cuda.is_available()\n",
        "\n",
        "print(train_gpu)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YVn6qEcoLov"
      },
      "source": [
        "### Архитектура нейронной сети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMGb4xZTi_h5"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class LSTM_architecture(nn.Module):\n",
        "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, number_of_layers, drop=0.5):\n",
        "        super(LSTM_architecture, self).__init__()\n",
        "        self.output_size = output_size\n",
        "        self.number_of_layers = number_of_layers\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, number_of_layers,dropout=drop, batch_first=True)\n",
        "        self.dropout = nn.Dropout(0.45)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x, hidden_state):\n",
        "        lstm_out, hidden_state = self.lstm(self.embedding(x.long()), hidden_state)\n",
        "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
        "        out = self.dropout(lstm_out)\n",
        "        out = self.fc(out)\n",
        "        sig_out = self.sigmoid(out)\n",
        "        batch_size = x.size(0)\n",
        "        sig_out = sig_out.view(batch_size, -1)\n",
        "        sig_out = sig_out[:, -1]\n",
        "        return sig_out, hidden_state\n",
        "\n",
        "    def init_hidden_state(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden_state = (weight.new(self.number_of_layers, batch_size, self.hidden_dim).zero_(),\n",
        "                      weight.new(self.number_of_layers, batch_size, self.hidden_dim).zero_())\n",
        "        return hidden_state\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d22xLEVf9kw"
      },
      "source": [
        "###Выбор гиперпараметров и инициализация сети"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vb8ceq4OjFW-"
      },
      "source": [
        "vocab_size = len(vocab_to_int)+1\n",
        "output_size = 1\n",
        "embedding_dim = 128\n",
        "hidden_dim = 128\n",
        "n_layers = 2\n",
        "model = LSTM_architecture(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhuHQKxlFsn1",
        "outputId": "bc8dba89-c28e-4179-a61e-d6af35da1963"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "111797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ty7J6NXZhn-N"
      },
      "source": [
        "### Обучение модели"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXTBYHCcjHy9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b07a62d7-efae-4527-ff06-31b9aa8f4583"
      },
      "source": [
        "test_res=[]\n",
        "epochs = 3 #оптимальное количество эпох для того, чтобы модель достаточно обучилась, но не переобучилась\n",
        "counter = 0\n",
        "batch_num = 100\n",
        "clip=5\n",
        "if(train_gpu):\n",
        "    model.cuda()\n",
        "num_correct = 0\n",
        "model.train()\n",
        "for e in range(epochs):\n",
        "    h = model.init_hidden_state(batch_size)\n",
        "    for inputs, labels in train_loader:\n",
        "        num_correct = 0\n",
        "        counter += 1\n",
        "        if(train_gpu):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "        h = tuple([each.data for each in h])\n",
        "        model.zero_grad()\n",
        "        output, h = model.forward(inputs, h)\n",
        "        loss = criterion(output.squeeze(), labels.float())\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        if counter % batch_num == 0:\n",
        "            val_h = model.init_hidden_state(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            for inputs, labels in valid_loader:\n",
        "                val_h = tuple([each.data for each in val_h])\n",
        "                if(train_gpu):\n",
        "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
        "                output, val_h = model(inputs, val_h)\n",
        "                val_loss = criterion(output.squeeze(), labels.float())\n",
        "                val_losses.append(val_loss.item())\n",
        "\n",
        "                #accuracy\n",
        "                pred = torch.round(output.squeeze())\n",
        "                correct_tensor = pred.eq(labels.float().view_as(pred))\n",
        "                correct = np.squeeze(correct_tensor.numpy()) if not train_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
        "                num_correct += np.sum(correct)\n",
        "                valid_acc = num_correct/len(valid_loader.dataset)\n",
        "\n",
        "            model.train()\n",
        "            print(\"Epoch: {} ;\".format(e+1),\n",
        "                  \"Batch Number: {};\".format(counter),\n",
        "                  \"Train Loss: {:.4f} ;\".format(loss.item()),\n",
        "                  \"Valid Loss: {:.4f} ;\".format(np.mean(val_losses)),\n",
        "                  \"Valid Accuracy: {:.4f}\".format(valid_acc))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 ; Batch Number: 100; Train Loss: 0.1970 ; Valid Loss: 0.3006 ; Valid Accuracy: 0.8769\n",
            "Epoch: 1 ; Batch Number: 200; Train Loss: 0.2082 ; Valid Loss: 0.2851 ; Valid Accuracy: 0.8835\n",
            "Epoch: 1 ; Batch Number: 300; Train Loss: 0.2193 ; Valid Loss: 0.2767 ; Valid Accuracy: 0.8846\n",
            "Epoch: 1 ; Batch Number: 400; Train Loss: 0.3086 ; Valid Loss: 0.2596 ; Valid Accuracy: 0.8958\n",
            "Epoch: 1 ; Batch Number: 500; Train Loss: 0.2009 ; Valid Loss: 0.2519 ; Valid Accuracy: 0.8999\n",
            "Epoch: 1 ; Batch Number: 600; Train Loss: 0.3116 ; Valid Loss: 0.2579 ; Valid Accuracy: 0.9008\n",
            "Epoch: 1 ; Batch Number: 700; Train Loss: 0.2416 ; Valid Loss: 0.2469 ; Valid Accuracy: 0.9002\n",
            "Epoch: 1 ; Batch Number: 800; Train Loss: 0.2307 ; Valid Loss: 0.2493 ; Valid Accuracy: 0.9015\n",
            "Epoch: 1 ; Batch Number: 900; Train Loss: 0.1625 ; Valid Loss: 0.2372 ; Valid Accuracy: 0.9062\n",
            "Epoch: 1 ; Batch Number: 1000; Train Loss: 0.4734 ; Valid Loss: 0.2381 ; Valid Accuracy: 0.9051\n",
            "Epoch: 1 ; Batch Number: 1100; Train Loss: 0.2199 ; Valid Loss: 0.2405 ; Valid Accuracy: 0.9040\n",
            "Epoch: 1 ; Batch Number: 1200; Train Loss: 0.1904 ; Valid Loss: 0.2319 ; Valid Accuracy: 0.9093\n",
            "Epoch: 2 ; Batch Number: 1300; Train Loss: 0.1026 ; Valid Loss: 0.2392 ; Valid Accuracy: 0.9040\n",
            "Epoch: 2 ; Batch Number: 1400; Train Loss: 0.2020 ; Valid Loss: 0.2365 ; Valid Accuracy: 0.9050\n",
            "Epoch: 2 ; Batch Number: 1500; Train Loss: 0.1582 ; Valid Loss: 0.2394 ; Valid Accuracy: 0.9094\n",
            "Epoch: 2 ; Batch Number: 1600; Train Loss: 0.1753 ; Valid Loss: 0.2409 ; Valid Accuracy: 0.9100\n",
            "Epoch: 2 ; Batch Number: 1700; Train Loss: 0.3378 ; Valid Loss: 0.2413 ; Valid Accuracy: 0.9067\n",
            "Epoch: 2 ; Batch Number: 1800; Train Loss: 0.1440 ; Valid Loss: 0.2280 ; Valid Accuracy: 0.9086\n",
            "Epoch: 2 ; Batch Number: 1900; Train Loss: 0.2282 ; Valid Loss: 0.2331 ; Valid Accuracy: 0.9107\n",
            "Epoch: 2 ; Batch Number: 2000; Train Loss: 0.4016 ; Valid Loss: 0.2289 ; Valid Accuracy: 0.9135\n",
            "Epoch: 2 ; Batch Number: 2100; Train Loss: 0.3544 ; Valid Loss: 0.2368 ; Valid Accuracy: 0.9123\n",
            "Epoch: 2 ; Batch Number: 2200; Train Loss: 0.1590 ; Valid Loss: 0.2235 ; Valid Accuracy: 0.9120\n",
            "Epoch: 2 ; Batch Number: 2300; Train Loss: 0.2812 ; Valid Loss: 0.2210 ; Valid Accuracy: 0.9131\n",
            "Epoch: 2 ; Batch Number: 2400; Train Loss: 0.1897 ; Valid Loss: 0.2217 ; Valid Accuracy: 0.9127\n",
            "Epoch: 2 ; Batch Number: 2500; Train Loss: 0.0958 ; Valid Loss: 0.2193 ; Valid Accuracy: 0.9141\n",
            "Epoch: 3 ; Batch Number: 2600; Train Loss: 0.1552 ; Valid Loss: 0.2457 ; Valid Accuracy: 0.9101\n",
            "Epoch: 3 ; Batch Number: 2700; Train Loss: 0.0931 ; Valid Loss: 0.2322 ; Valid Accuracy: 0.9116\n",
            "Epoch: 3 ; Batch Number: 2800; Train Loss: 0.1581 ; Valid Loss: 0.2346 ; Valid Accuracy: 0.9134\n",
            "Epoch: 3 ; Batch Number: 2900; Train Loss: 0.1637 ; Valid Loss: 0.2369 ; Valid Accuracy: 0.9160\n",
            "Epoch: 3 ; Batch Number: 3000; Train Loss: 0.1655 ; Valid Loss: 0.2319 ; Valid Accuracy: 0.9111\n",
            "Epoch: 3 ; Batch Number: 3100; Train Loss: 0.1268 ; Valid Loss: 0.2682 ; Valid Accuracy: 0.9077\n",
            "Epoch: 3 ; Batch Number: 3200; Train Loss: 0.1174 ; Valid Loss: 0.2368 ; Valid Accuracy: 0.9087\n",
            "Epoch: 3 ; Batch Number: 3300; Train Loss: 0.1333 ; Valid Loss: 0.2346 ; Valid Accuracy: 0.9146\n",
            "Epoch: 3 ; Batch Number: 3400; Train Loss: 0.0311 ; Valid Loss: 0.2429 ; Valid Accuracy: 0.9147\n",
            "Epoch: 3 ; Batch Number: 3500; Train Loss: 0.2077 ; Valid Loss: 0.2626 ; Valid Accuracy: 0.9050\n",
            "Epoch: 3 ; Batch Number: 3600; Train Loss: 0.1849 ; Valid Loss: 0.2489 ; Valid Accuracy: 0.9076\n",
            "Epoch: 3 ; Batch Number: 3700; Train Loss: 0.1760 ; Valid Loss: 0.2448 ; Valid Accuracy: 0.9110\n",
            "Epoch: 3 ; Batch Number: 3800; Train Loss: 0.2118 ; Valid Loss: 0.2368 ; Valid Accuracy: 0.9111\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNNThCsgpe-o"
      },
      "source": [
        "### Сентимент-анализ"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HxNOKY5qCmJ"
      },
      "source": [
        "def tokenize_text(test_review):\n",
        "    test_review = test_review.lower()\n",
        "    punctuation = '!\"#$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~№'\n",
        "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
        "    test_words = test_text.split()\n",
        "    prep_test = []\n",
        "    for token in test_words:\n",
        "        lemma = morph.parse(token)[0].normal_form\n",
        "        if lemma not in rus_stopwords:\n",
        "            prep_test.append(lemma)\n",
        "    new_text = []\n",
        "    for word in prep_test:\n",
        "        if (word[0] != '@') & ('http' not in word) & (~word.isdigit()):\n",
        "            new_text.append(word)\n",
        "    test_ints = []\n",
        "    test_ints.append([vocab_to_int[word] for word in new_text])\n",
        "\n",
        "    return test_ints"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "split_id = 64000\n",
        "trained_x, remained_x = reviews[:split_id], reviews[split_id:]\n",
        "trained_y, remained_y = labels[:split_id], labels[split_id:]\n",
        "test_id = int(len(remained_x)*0.5)\n",
        "valed_x, tested_x = remained_x[:test_id], remained_x[test_id:]\n",
        "valed_y, tested_y = remained_y[:test_id], remained_y[test_id:]"
      ],
      "metadata": {
        "id": "LY8hJ-0zqA_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN7jtCPrqKX2"
      },
      "source": [
        "def predict(model, test_review, test_res, sequence_length=50):\n",
        "    model.eval()\n",
        "    test_ints = tokenize_text(test_review)\n",
        "    seq_length=sequence_length\n",
        "    features = add_pads(test_ints, seq_length)\n",
        "    feature_tensor = torch.from_numpy(features)\n",
        "    batch_size = feature_tensor.size(0)\n",
        "    h = model.init_hidden_state(batch_size)\n",
        "    if(train_gpu):\n",
        "        feature_tensor = feature_tensor.cuda()\n",
        "    output, h = model(feature_tensor, h)\n",
        "\n",
        "    pred = torch.round(output.squeeze())\n",
        "    print('Вероятность положительного ответа {:.6f}'.format(output.item()))\n",
        "\n",
        "    if(pred.item()==1):\n",
        "        print(\"Позитивное сообщение\")\n",
        "    else:\n",
        "        print(\"Негативное сообщение\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWH06tObqP21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c29276a1-2761-494d-b6e6-c9d656fba0f2"
      },
      "source": [
        "seq_length = 30\n",
        "test_review = \"хороший день сегодня\"\n",
        "predict(model, test_review, seq_length)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Вероятность положительного ответа 0.878721\n",
            "Позитивное сообщение\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = \"model\"\n",
        "torch.save(model.state_dict(), PATH)"
      ],
      "metadata": {
        "id": "hENlKlRsIx3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QxtA34hGIxpA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}